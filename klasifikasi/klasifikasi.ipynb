{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang dibutuhkan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed untuk reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================================================\n",
    "# 1. LOAD DATA DAN EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ===============================================================\n",
    "print(\"1. Loading data and performing EDA...\")\n",
    "\n",
    "# Load dataset\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', \n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', \n",
    "                'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "# Load dataset (adjust path as needed)\n",
    "df = pd.read_csv('adult.data', names=column_names, skipinitialspace=True, na_values=\"?\")\n",
    "\n",
    "# Dataset Overview\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(df.info())\n",
    "\n",
    "# Descriptive Statistics\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Categorical Statistics\n",
    "print(\"\\nCategorical Statistics:\")\n",
    "print(df.describe(include=['object']))\n",
    "\n",
    "# Check Missing Values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values)\n",
    "print(\"\\nMissing Percentage:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Target Distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(df['income'].value_counts(normalize=True))\n",
    "\n",
    "# ===============================================================\n",
    "# 2. VISUALISASI DATA\n",
    "# ===============================================================\n",
    "print(\"\\n2. Data Visualization...\")\n",
    "\n",
    "# Histogram untuk variabel numerik\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_features].hist(figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.savefig('numeric_histograms.png')\n",
    "plt.close()\n",
    "\n",
    "# Bar plot untuk variabel kategorikal\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    df[feature].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(feature)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'distribution_{feature}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Korelasi antara variabel numerik\n",
    "correlation_matrix = df[numeric_features].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# ===============================================================\n",
    "# 3. DATA CLEANING\n",
    "# ===============================================================\n",
    "print(\"\\n3. Data Cleaning...\")\n",
    "\n",
    "# Handle missing values\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        # For categorical columns, fill with mode\n",
    "        df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "    else:\n",
    "        # For numeric columns, fill with median\n",
    "        df[column].fillna(df[column].median(), inplace=True)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Number of rows after removing duplicates: {len(df)}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 4. FEATURE ENGINEERING & ENCODING\n",
    "# ===============================================================\n",
    "print(\"\\n4. Feature Engineering & Encoding...\")\n",
    "\n",
    "# Store encoding maps\n",
    "encoding_maps = {}\n",
    "\n",
    "# Encoding for 'workclass'\n",
    "workclass_map = {'State-gov':5, 'Self-emp-not-inc':6, 'Private':7, 'Federal-gov':4, 'Local-gov':3,\n",
    "                'Self-emp-inc':2, 'Without-pay':1, 'Never-worked':0}\n",
    "df['workclass'] = df['workclass'].map(workclass_map)\n",
    "encoding_maps['workclass'] = workclass_map\n",
    "\n",
    "# Encoding for 'marital_status'\n",
    "marital_status_map = {'Never-married':0, 'Married-civ-spouse':1, 'Divorced':2, 'Married-spouse-absent':3,\n",
    "                      'Separated':6, 'Married-AF-spouse':5, 'Widowed':4}\n",
    "df['marital_status'] = df['marital_status'].map(marital_status_map)\n",
    "encoding_maps['marital_status'] = marital_status_map\n",
    "\n",
    "# Encoding for 'occupation'\n",
    "occupation_map = {'Adm-clerical':1, 'Exec-managerial':2, 'Handlers-cleaners':3, 'Prof-specialty':4,\n",
    "                 'Other-service':5, 'Sales':6, 'Craft-repair':7, 'Transport-moving':8,\n",
    "                 'Farming-fishing':9, 'Machine-op-inspct':10, 'Tech-support':11,\n",
    "                 'Protective-serv':12, 'Armed-Forces':13, 'Priv-house-serv':14}\n",
    "df['occupation'] = df['occupation'].map(occupation_map)\n",
    "encoding_maps['occupation'] = occupation_map\n",
    "\n",
    "# Encoding for 'relationship'\n",
    "relationship_map = {'Not-in-family':1, 'Husband':2, 'Wife':3, 'Own-child':4, 'Unmarried':0, 'Other-relative':5}\n",
    "df['relationship'] = df['relationship'].map(relationship_map)\n",
    "encoding_maps['relationship'] = relationship_map\n",
    "\n",
    "# Encoding for 'race'\n",
    "race_map = {'White':1, 'Black':2, 'Asian-Pac-Islander':3, 'Amer-Indian-Eskimo':4, 'Other':5}\n",
    "df['race'] = df['race'].map(race_map)\n",
    "encoding_maps['race'] = race_map\n",
    "\n",
    "# Encoding for 'sex'\n",
    "sex_map = {'Male':1, 'Female':0}\n",
    "df['sex'] = df['sex'].map(sex_map)\n",
    "encoding_maps['sex'] = sex_map\n",
    "\n",
    "# Encoding for 'native_country'\n",
    "native_country_map = {'United-States':1, 'Cuba':2, 'Jamaica':3, 'India':4, 'Mexico':5, 'South':6,\n",
    "                      'Puerto-Rico':6, 'Honduras':7, 'England':8, 'Canada':9, 'Germany':10, 'Iran':11,\n",
    "                      'Philippines':12, 'Italy':13, 'Poland':14, 'Columbia':15, 'Cambodia':16, 'Thailand':17, 'Ecuador':18,\n",
    "                      'Laos':19, 'Taiwan':20, 'Haiti':21, 'Portugal':22, 'Dominican-Republic':23, 'El-Salvador':24,\n",
    "                      'France':25, 'Guatemala':26, 'China':27, 'Japan':28, 'Yugoslavia':29, 'Peru':30,\n",
    "                      'Outlying-US(Guam-USVI-etc)':31, 'Scotland':32, 'Trinadad&Tobago':33, 'Greece':34,\n",
    "                      'Nicaragua':35, 'Vietnam':36, 'Hong':37, 'Ireland':38, 'Hungary':39, 'Holand-Netherlands':40}\n",
    "df['native_country'] = df['native_country'].map(native_country_map)\n",
    "encoding_maps['native_country'] = native_country_map\n",
    "\n",
    "# Encoding for 'income' (target)\n",
    "income_map = {'<=50K':0, '>50K':1}\n",
    "df['income'] = df['income'].map(income_map)\n",
    "encoding_maps['income'] = income_map\n",
    "\n",
    "# Drop education column (since education_num is already available)\n",
    "df = df.drop('education', axis=1)\n",
    "\n",
    "# ===============================================================\n",
    "# 5. CHECKING CORRELATIONS AFTER ENCODING\n",
    "# ===============================================================\n",
    "print(\"\\n5. Checking correlations after encoding...\")\n",
    "\n",
    "# Create correlation heatmap\n",
    "correlation = df.corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(correlation.round(2),\n",
    "           annot=True,\n",
    "           vmax=1,\n",
    "           square=True,\n",
    "           cmap='RdYlGn_r')\n",
    "plt.title('Correlation Matrix After Encoding')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_after_encoding.png')\n",
    "plt.close()\n",
    "\n",
    "# ===============================================================\n",
    "# 6. FEATURE SELECTION\n",
    "# ===============================================================\n",
    "print(\"\\n6. Feature Selection...\")\n",
    "\n",
    "# Remove constant features\n",
    "df = df.loc[:, df.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "# Find and remove highly correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "data_tanpa_fitur = df.drop('income', axis=1)\n",
    "corr_features = correlation(data_tanpa_fitur, 0.8)\n",
    "print('Correlated features: ', len(set(corr_features)))\n",
    "print(corr_features)\n",
    "\n",
    "# Remove highly correlated features\n",
    "df.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "# Final correlation check\n",
    "correlation = df.corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(correlation.round(2),\n",
    "           annot=True,\n",
    "           vmax=1,\n",
    "           square=True,\n",
    "           cmap='RdYlGn_r')\n",
    "plt.title('Final Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_correlation.png')\n",
    "plt.close()\n",
    "\n",
    "# ===============================================================\n",
    "# 7. MODEL TRAINING\n",
    "# ===============================================================\n",
    "print(\"\\n7. Model Training...\")\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('income', axis=1)\n",
    "y = df['income']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Create decision tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 8. MODEL EVALUATION\n",
    "# ===============================================================\n",
    "print(\"\\n8. Model Evaluation...\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=sorted(df['income'].unique()),\n",
    "           yticklabels=sorted(df['income'].unique()))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Visualization of the Decision Tree\n",
    "plt.figure(figsize=(40, 30))\n",
    "plot_tree(best_model,\n",
    "         feature_names=X.columns,\n",
    "         class_names=[str(i) for i in sorted(df['income'].unique())],\n",
    "         filled=True,\n",
    "         rounded=True,\n",
    "         max_depth=3)  # Limiting depth for better visualization\n",
    "plt.title('Decision Tree Visualization (Limited to Depth 3)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('decision_tree.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Check performance on training data\n",
    "ori_y_pred_dt_train = best_model.predict(X_train)\n",
    "\n",
    "ori_accuracy_dt_train = accuracy_score(y_train, ori_y_pred_dt_train)\n",
    "print('Akurasi pada training set: ', ori_accuracy_dt_train)\n",
    "\n",
    "ori_precision_dt_train = precision_score(y_train, ori_y_pred_dt_train, average='micro')\n",
    "print('Precision pada training set: ', ori_precision_dt_train)\n",
    "\n",
    "ori_recall_dt_train = recall_score(y_train, ori_y_pred_dt_train, average='micro')\n",
    "print('Recall pada training set: ', ori_recall_dt_train)\n",
    "\n",
    "# Recheck performance on testing data\n",
    "ori_accuracy_dt_test = accuracy_score(y_test, y_pred)\n",
    "print('Akurasi pada test set: ', ori_accuracy_dt_test)\n",
    "\n",
    "ori_precision_dt_test = precision_score(y_test, y_pred, average='micro')\n",
    "print('Precision pada test set: ', ori_precision_dt_test)\n",
    "\n",
    "ori_recall_dt_test = recall_score(y_test, y_pred, average='micro')\n",
    "print('Recall pada test set: ', ori_recall_dt_test)\n",
    "\n",
    "# ===============================================================\n",
    "# 9. CREATE MODEL COMPONENTS FOR SAVING\n",
    "# ===============================================================\n",
    "print(\"\\n9. Creating and saving model components...\")\n",
    "\n",
    "# Create a dictionary of components\n",
    "model_components = {\n",
    "    'model': best_model,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'encoding_maps': encoding_maps,\n",
    "    'model_params': best_params,\n",
    "    'removed_features': list(corr_features) if len(corr_features) > 0 else [],\n",
    "    'target_map': income_map\n",
    "}\n",
    "\n",
    "# Save model components\n",
    "joblib.dump(model_components, 'income_prediction_components.joblib')\n",
    "print(\"Model components saved successfully as 'income_prediction_components.joblib'\")\n",
    "\n",
    "# Function to predict with the model (for testing)\n",
    "def predict_income(data, model_components):\n",
    "    \"\"\"\n",
    "    Make income predictions using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict or DataFrame\n",
    "        Data with features for prediction\n",
    "    model_components : dict\n",
    "        Dictionary containing model and preprocessing information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    prediction : int \n",
    "        Predicted income class (0 for '<=50K', 1 for '>50K')\n",
    "    probability : float\n",
    "        Probability of the predicted class\n",
    "    \"\"\"\n",
    "    # Convert single record to DataFrame if needed\n",
    "    if isinstance(data, dict):\n",
    "        data = pd.DataFrame([data])\n",
    "    \n",
    "    # Get components\n",
    "    model = model_components['model']\n",
    "    encoding_maps = model_components['encoding_maps']\n",
    "    feature_names = model_components['feature_names']\n",
    "    \n",
    "    # Apply encoding to categorical features\n",
    "    for col in data.columns:\n",
    "        if col in encoding_maps and col != 'income':\n",
    "            data[col] = data[col].map(encoding_maps[col])\n",
    "    \n",
    "    # Ensure we only use features that the model was trained on\n",
    "    data_for_pred = data[feature_names].copy()\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(data_for_pred)[0]\n",
    "    probabilities = model.predict_proba(data_for_pred)[0]\n",
    "    \n",
    "    # Get inverse mapping for income\n",
    "    income_map_inverse = {v: k for k, v in encoding_maps['income'].items()}\n",
    "    prediction_label = income_map_inverse[prediction]\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'prediction_label': prediction_label,\n",
    "        'probability': probabilities[prediction]\n",
    "    }\n",
    "\n",
    "# Test the prediction function with a sample\n",
    "test_sample = X_test.iloc[0].to_dict()\n",
    "loaded_components = joblib.load('income_prediction_components.joblib')\n",
    "prediction_result = predict_income(test_sample, loaded_components)\n",
    "print(\"\\nTest prediction result:\")\n",
    "print(prediction_result)\n",
    "print(\"Actual value:\", y_test.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
